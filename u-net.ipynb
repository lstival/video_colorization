{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from ViT import *\n",
    "\n",
    "# Loss imports\n",
    "from piq import ssim, SSIMLoss\n",
    "\n",
    "# Images of training epochs\n",
    "if not os.path.exists('./dc_img'):\n",
    "    os.mkdir('./dc_img')\n",
    "\n",
    "# Models Save\n",
    "if not os.path.exists('./models'):\n",
    "    os.mkdir('./models')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 256, 256)\n",
    "    return x\n",
    "\n",
    "\n",
    "# batch_size = 128\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "import load_data as ld\n",
    "dataLoader = ld.ReadData()\n",
    "import torch\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "image_size = (256, 256)\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "# Root directory for dataset\n",
    "dataroot = \"../data/train/sunset\"\n",
    "\n",
    "dataloader = dataLoader.create_dataLoader(dataroot, image_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_clor = next(iter(dataloader))[0]\n",
    "img = img_clor[:,:1,:,:]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 128, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(1, 16, 3, stride=2, padding=1)(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MaxPool2d(2, stride=2)(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvDown2d(nn.Module):\n",
    "#     def __init__(self, in_channel, out_channel, kernel_size=3, stride=2, padding=1) -> None:\n",
    "#         super(ConvDown2d, self).__init__()\n",
    "#         self.in_channel = in_channel\n",
    "#         self.out_channel = out_channel\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "\n",
    "#     def forward(self, x) -> torch.Tensor:\n",
    "#         conv = nn.Sequential(\n",
    "#             nn.Conv2d(self.in_channel, self.out_channel, self.kernel_size, self.stride, self.padding),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(2, stride=1)\n",
    "#         )\n",
    "#         return conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv = ConvDown2d(1,16,3,2,1)\n",
    "# conv(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvDown2d(in_channel, out_channel, kernel_size=3, stride=2, padding=1):\n",
    "\n",
    "    conv = nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding)\n",
    "    return conv\n",
    "\n",
    "def ConvUp2d(in_channel, out_channel, kernel_size=3, stride=2, padding=1):\n",
    "\n",
    "    conv = nn.ConvTranspose2d(in_channel, out_channel, kernel_size, stride, padding)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride, padding) -> None:\n",
    "        super(ColorNetwork, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.kernel_size = 3\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        #Encoder Network\n",
    "        self.dw_conv1 = ConvDown2d(1, self.out_channel, self.kernel_size,self.stride,self.padding)\n",
    "        self.max_pol1 = nn.MaxPool2d(2, stride=1)\n",
    "\n",
    "        self.dw_conv2 = ConvDown2d(self.out_channel, self.out_channel*2, self.kernel_size,self.stride,self.padding)\n",
    "        self.max_pol2 = nn.MaxPool2d(2, stride=1)\n",
    "\n",
    "        self.dw_conv3 = ConvDown2d(self.out_channel*2, self.out_channel*4, self.kernel_size,self.stride,self.padding)\n",
    "        self.max_pol3 = nn.MaxPool2d(2, stride=1)\n",
    "\n",
    "        #Decoder\n",
    "        self.up_conv1 = ConvUp2d(self.out_channel*8, self.out_channel*2, 2, self.stride, 0)\n",
    "\n",
    "        self.up_conv2 = ConvUp2d(self.out_channel*4, self.out_channel, 2, self.stride,0)\n",
    "\n",
    "        self.up_conv3 = ConvUp2d(self.out_channel*2, 3, 2, 2, 0)\n",
    "\n",
    "        #Activation\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, color_sample) -> torch.Tensor:\n",
    "\n",
    "        #Encoder\n",
    "        e1 = self.dw_conv1(x)\n",
    "        e1 = nn.ReLU(True)(e1)\n",
    "        e1 = self.max_pol1(e1)\n",
    "        print(f\"e1 shape: {e1.shape}\")\n",
    "\n",
    "        e2 = self.dw_conv2(e1)\n",
    "        e2 = nn.ReLU(True)(e2)\n",
    "        e2 = self.max_pol2(e2)\n",
    "        print(f\"e2 shape: {e2.shape}\")\n",
    "\n",
    "        e3 = self.dw_conv3(e2)\n",
    "        e3 = nn.ReLU(True)(e3)\n",
    "        e3 = self.max_pol3(e3)\n",
    "        print(f\"e3 shape: {e3.shape}\")\n",
    "\n",
    "        #BottlerNeck\n",
    "        # neck = vit.forward(color_sample)\n",
    "        # print(f\"neck shape: {neck.shape}\")\n",
    "        # neck = torch.reshape(neck, (e3.shape[0], e3.shape[1], e3.shape[2], e3.shape[3]))\n",
    "        neck = e3\n",
    "        \n",
    "        #Decoder\n",
    "        e3 = torch.cat((neck, e3), 1)\n",
    "        d1 = self.up_conv1(e3)\n",
    "        d1 = nn.ReLU(True)(d1)\n",
    "        print(f\"d1 shape: {d1.shape}\")\n",
    "\n",
    "        d2 = torch.cat((e2, d1), 1)\n",
    "        d2 = self.up_conv2(d2)\n",
    "        d2 = nn.ReLU(True)(d2)\n",
    "        print(f\"d2 shape: {d2.shape}\")\n",
    "\n",
    "        d3 = torch.cat((e1, d2), 1)\n",
    "        d3 = self.up_conv3(d3)\n",
    "        d3 = nn.ReLU(True)(d3)\n",
    "        print(f\"d3 shape: {d3.shape}\")\n",
    "\n",
    "        #Activation\n",
    "        out = self.activation(d3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual Transformer\n",
    "from ViT import *\n",
    "#Visual Transformer\n",
    "vit = Vit_neck(batch_size, image_size[0], 64*32*32)\n",
    "\n",
    "a = torch.rand(batch_size, 256, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 256, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_clor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1 shape: torch.Size([32, 32, 128, 128])\n",
      "e2 shape: torch.Size([32, 64, 64, 64])\n",
      "e3 shape: torch.Size([32, 128, 32, 32])\n",
      "d1 shape: torch.Size([32, 64, 64, 64])\n",
      "d2 shape: torch.Size([32, 32, 128, 128])\n",
      "d3 shape: torch.Size([32, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 256, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ColorNetwork(1,32,2,2)\n",
    "net(img, img_clor.to(\"cuda\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1 shape: torch.Size([32, 32, 128, 128])\n",
      "e2 shape: torch.Size([32, 64, 64, 64])\n",
      "e3 shape: torch.Size([32, 128, 32, 32])\n",
      "d1 shape: torch.Size([32, 64, 64, 64])\n",
      "d2 shape: torch.Size([32, 32, 128, 128])\n",
      "d3 shape: torch.Size([32, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 256, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ColorNetwork(1,32,2,2)\n",
    "net(img, img_clor.to(\"cuda\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = torch.ones(256,256)\n",
    "c2 = torch.add(c1, 1)\n",
    "c3 = torch.add(c2, 1)\n",
    "# c2 = torch.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.concat((c1.unsqueeze(0),c2.unsqueeze(0),c3.unsqueeze(0)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[:1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How make skip connection in pytorch\n",
    "# https://github.com/pytorch/vision/blob/a9a8220e0bcb4ce66a733f8c03a1c2f6c68d22cb/torchvision/models/resnet.py#L56-L72"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gcn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c8146f26135f1c926afd56a5c89149aa7350eae42937c6df54039f048d566a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
